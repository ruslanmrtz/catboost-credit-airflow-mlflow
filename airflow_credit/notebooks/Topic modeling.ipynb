{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-youtube mlflow pymystem3 nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "from pyyoutube import Api\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import f1_score, silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(YOUTUBE_API_KEY, videoId, maxResults, nextPageToken):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤–∏–¥–µ–æ\n",
    "    \"\"\"\n",
    "    YOUTUBE_URI = 'https://www.googleapis.com/youtube/v3/commentThreads?key={KEY}&textFormat=plainText&' + \\\n",
    "        'part=snippet&videoId={videoId}&maxResults={maxResults}&pageToken={nextPageToken}'\n",
    "    format_youtube_uri = YOUTUBE_URI.format(KEY=YOUTUBE_API_KEY,\n",
    "                                            videoId=videoId,\n",
    "                                            maxResults=maxResults,\n",
    "                                            nextPageToken=nextPageToken)\n",
    "    content = requests.get(format_youtube_uri).text\n",
    "    data = json.loads(content)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_text_of_comment(data):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –æ–¥–Ω–∏–º –≤–∏–¥–µ–æ\n",
    "    \"\"\"\n",
    "    comms = set()\n",
    "    for item in data['items']:\n",
    "        comm = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        comms.add(comm)\n",
    "    return comms\n",
    "\n",
    "\n",
    "def get_all_comments(YOUTUBE_API_KEY, query, count_video=10, limit=30, maxResults=10, nextPageToken=''):\n",
    "    \"\"\"\n",
    "    –í—ã–≥—Ä—É–∑–∫–∞ maxResults –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\n",
    "    \"\"\"\n",
    "    api = Api(api_key=YOUTUBE_API_KEY)\n",
    "    video_by_keywords = api.search_by_keywords(q=query,\n",
    "                                               search_type=[\"video\"],\n",
    "                                               count=count_video,\n",
    "                                               limit=limit)\n",
    "    videoId = [x.id.videoId for x in video_by_keywords.items]\n",
    "\n",
    "    comments_all = []\n",
    "    for id_video in videoId:\n",
    "        try:\n",
    "            data = get_data(YOUTUBE_API_KEY,\n",
    "                            id_video,\n",
    "                            maxResults=maxResults,\n",
    "                            nextPageToken=nextPageToken)\n",
    "            comment = list(get_text_of_comment(data))\n",
    "            comments_all.append(comment)\n",
    "        except:\n",
    "            continue\n",
    "    comments = sum(comments_all, [])\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join('../config/params_all.yaml')\n",
    "config = yaml.safe_load(open(config_path))['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = config['SEED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEED': 10,\n",
       " 'clustering': {'affinity': 'cosine',\n",
       "  'count_max_clusters': 15,\n",
       "  'silhouette_metric': 'euclidean'},\n",
       " 'comments': {'YOUTUBE_API_KEY': 'AIzaSyCfZBiaYvhsHf7NDHANcp6zFO57z0cX-L8',\n",
       "  'count_video': 50,\n",
       "  'limit': 30,\n",
       "  'maxResults': 5,\n",
       "  'nextPageToken': '',\n",
       "  'query': 'Data Science'},\n",
       " 'cross_val': {'test_size': 0.3},\n",
       " 'dir_folder': '/Users/miracl6/airflow-mlflow-tutorial',\n",
       " 'model': {'class_weight': 'balanced'},\n",
       " 'model_lr': 'LogisticRegression',\n",
       " 'model_vec': 'vector_tfidf',\n",
       " 'name_experiment': 'my_third',\n",
       " 'stopwords': 'russian',\n",
       " 'tf_model': {'max_features': 300}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = get_all_comments(**config['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@ApnaCollegeOfficial , Respected shraddha di please tell me about eligibility criteria to admission to your esteemed curriculum??',\n",
       " 'Mam kya diploma mechanical wale student data science mei job le sakte hai kya',\n",
       " 'So Difficult',\n",
       " 'how we can create project?',\n",
       " 'Is she software engineer???',\n",
       " 'this explanation is really very easy to understand',\n",
       " 'üëçüèª',\n",
       " \"I came a long way i won't stop studying Medical school made me happy curing myself on my own is terrible satisfying inject yourself with the drug you made\",\n",
       " 'I have a test on this only',\n",
       " 'Inject yourself...inject yourself',\n",
       " 'The video is so helpful and supportive. Thank you so much',\n",
       " '\"üî•Data Scientist Masters Program (Discount Code - YTBE15) - https://www.simplilearn.com/big-data-and-analytics/senior-data-scientist-masters-program-training?utm_campaign=X3paOmcrTjQ&utm_medium=Comments&utm_source=Youtube\\nüî•IITK - Professional Certificate Course in Data Science (India Only) - https://www.simplilearn.com/iitk-professional-certificate-course-data-science?utm_campaign=X3paOmcrTjQ&utm_medium=Comments&utm_source=Youtube\\nüî•Caltech Post Graduate Program in Data Science  - https://www.simplilearn.com/post-graduate-program-data-science?utm_campaign=X3paOmcrTjQ&utm_medium=Comments&utm_source=Youtube\\nüî•Brown University - Applied AI & Data Science  - https://www.simplilearn.com/applied-ai-data-science-course?utm_campaign=X3paOmcrTjQ&utm_medium=Comments&utm_source=Youtube\"',\n",
       " \"Thank you soooo much you have helped me greatly in understanding this feild I was in a great amount of stress on what I should chose my older sister is also a data scientist and she told me I would be able to do it but I didn't know a single thing regarding this feild but thanks to your video and all the apps that you've recomended Inshallah I will be successfull in this Job\",\n",
       " 'I have one more query - why is feature engineering being taken under exploratory data analysis phase? This is a different phase altogether.',\n",
       " 'Can data science help in medical field']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    \"\"\"\n",
    "    –£–¥–∞–ª–µ–Ω–∏–µ —ç–º–æ–¥–∂–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "def remove_links(string):\n",
    "    \"\"\"\n",
    "    –£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫\n",
    "    \"\"\"\n",
    "    string = re.sub(r'http\\S+', '', string)  # remove http links\n",
    "    string = re.sub(r'bit.ly/\\S+', '', string)  # rempve bitly links\n",
    "    string = re.sub(r'www\\S+', '', string)  # rempve bitly links\n",
    "    string = string.strip('[link]')  # remove [links]\n",
    "    return string\n",
    "\n",
    "\n",
    "def preprocessing(string, stopwords, stem):\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞, –æ—á–∏—Å—Ç–∫–∞, –ª–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤\n",
    "    \"\"\"\n",
    "    string = remove_emoji(string)\n",
    "    string = remove_links(string)\n",
    "\n",
    "    # —É–¥–∞–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤ \"\\r\\n\"\n",
    "    str_pattern = re.compile(\"\\r\\n\")\n",
    "    string = str_pattern.sub(r'', string)\n",
    "\n",
    "    # –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å–∏–º–≤–æ–ª–æ–≤\n",
    "    string = re.sub('(((?![a-zA-Z ]).)+)', ' ', string)\n",
    "    # –ª–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "    string = ' '.join([\n",
    "        re.sub('\\\\n', '', ' '.join(stem.lemmatize(s))).strip()\n",
    "        for s in string.split()\n",
    "    ])\n",
    "    # —É–¥–∞–ª—è–µ–º —Å–ª–æ–≤–∞ –∫–æ—Ä–æ—á–µ 3 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "    string = ' '.join([s for s in string.split() if len(s) > 3])\n",
    "    # —É–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "    string = ' '.join([s for s in string.split() if s not in stopwords])\n",
    "    return string\n",
    "\n",
    "\n",
    "def get_clean_text(data, stopwords):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–π –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏\n",
    "    –º–∞—Ç—Ä–∏—á–Ω–æ–º –≤–∏–¥–µ, –∞ —Ç–∞–∫–∂–µ –º–æ–¥–µ–ª—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "    \"\"\"\n",
    "    # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞\n",
    "    stem = Mystem()\n",
    "    comments = [preprocessing(x, stopwords, stem) for x in data]\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –º–µ–Ω—å—à–µ, —á–µ–º 5 —Å–ª–æ–≤\n",
    "    comments = [y for y in comments if len(y.split()) > 5]\n",
    "    #common_texts = [i.split(' ') for i in comments]\n",
    "    return comments\n",
    "\n",
    "\n",
    "def vectorize_text(data, tfidf):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–ª-–≤–∞ —Å–ª–æ–≤ –≤ –∫–æ–º–º–µ–Ω–∞—Ä–∏—è—Ö\n",
    "    –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫\n",
    "    \"\"\"\n",
    "    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\n",
    "    X_matrix = tfidf.transform(data).toarray()\n",
    "    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≤ –º–∞—Ç—Ä–∏—Ü–µ —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n",
    "    mask = (np.nan_to_num(X_matrix) != 0).any(axis=1)\n",
    "    return X_matrix[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_clean = get_clean_text(comments, stopwords.words(config['stopwords']))\n",
    "tfidf = TfidfVectorizer(**config['tf_model']).fit(comments_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ApnaCollegeOfficial Respected shraddha please tell about eligibility criteria admission your esteemed curriculum',\n",
       " 'diploma mechanical wale student data science sakte',\n",
       " 'this explanation really very easy understand',\n",
       " 'came long stop studying Medical school made happy curing myself terrible satisfying inject yourself with drug made',\n",
       " 'Data Scientist Masters Program Discount Code YTBE IITK Professional Certificate Course Data Science India Only Caltech Post Graduate Program Data Science Brown University Applied Data Science',\n",
       " 'Thank soooo much have helped greatly understanding this feild great amount stress what should chose older sister also data scientist told would able didn know single thing regarding this feild thanks your video apps that recomended Inshallah will successfull this',\n",
       " 'have more query feature engineering being taken under exploratory data analysis phase This different phase altogether',\n",
       " 'Data analytics science rage computer science software engineering',\n",
       " 'Which best finance students study please',\n",
       " 'What difference between data science data analytics remember when started career even wasn entirely clear about distinction Back then thought both were just fancy terms working with data quickly realized different roles much each contributes uniquely solving problems This video such fantastic breakdown these disciplines loved highlighted that data analytics focuses finding insights answering specific questions while data science about building models uncovering patterns predictive insights nuanced distinction that confusing especially newcomers this explanation really mark during first project data science that difference truly clicked building machine learning model predict customer churn while colleague analyst digging into historical data understand churn happening first place Both tasks were critical they required completely different approaches tools ways thinking This kind collaboration what makes data world fascinating anyone watching this video trying decide between these paths think about whether enjoy diving deep into datasets find actionable insights data analytics rather experiment with algorithms code predictive models data science Either both fields incredibly rewarding videos like this great helping find your direction']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix = vectorize_text(comments_clean, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['about', 'accounts', 'advice', 'after', 'also', 'analysis',\n",
       "       'analyst', 'analysts', 'analytics', 'answer', 'answering',\n",
       "       'application'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(data, count_max_clusters, random_state, affinity,\n",
    "                 silhouette_metric):\n",
    "    \"\"\"\n",
    "    –ü–æ–¥–±–æ—Ä –Ω–∞–∏–ª—É—á—à–µ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ —Ç–µ–º–∞—Ç–∏–∫\n",
    "    \"\"\"\n",
    "    cluster_labels = {}\n",
    "    silhouette_mean = []\n",
    "\n",
    "    for i in range(2, count_max_clusters, 1):\n",
    "        clf = SpectralClustering(n_clusters=i,\n",
    "                                 affinity=affinity,\n",
    "                                 random_state=random_state)\n",
    "        #clf = KMeans(n_clusters=n, max_iter=1000, n_init=1)\n",
    "        clf.fit(data)\n",
    "        labels = clf.labels_\n",
    "        cluster_labels[i] = labels\n",
    "        silhouette_mean.append(\n",
    "            silhouette_score(data, labels, metric=silhouette_metric))\n",
    "    n_clusters = silhouette_mean.index(max(silhouette_mean)) + 2\n",
    "    return cluster_labels[n_clusters]\n",
    "\n",
    "\n",
    "def get_f1_score(y_test, y_pred, unique_cluster_labels):\n",
    "    \"\"\"\n",
    "    –í–æ–∑—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ø–æ —Ç–µ–º–∞—Ç–∏–∫–∞–º\n",
    "    \"\"\"\n",
    "    return f1_score(\n",
    "        y_test, y_pred,\n",
    "        average='macro') \\\n",
    "        if len(unique_cluster_labels) > 2 \\\n",
    "        else f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 1 sample(s) (shape=(1, 6)) while a minimum of 2 is required by SpectralClustering.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cluster_labels \u001b[38;5;241m=\u001b[39m get_clusters(X_matrix,\n\u001b[0;32m      2\u001b[0m                                  random_state\u001b[38;5;241m=\u001b[39mSEED,\n\u001b[0;32m      3\u001b[0m                                  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclustering\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[42], line 14\u001b[0m, in \u001b[0;36mget_clusters\u001b[1;34m(data, count_max_clusters, random_state, affinity, silhouette_metric)\u001b[0m\n\u001b[0;32m     10\u001b[0m clf \u001b[38;5;241m=\u001b[39m SpectralClustering(n_clusters\u001b[38;5;241m=\u001b[39mi,\n\u001b[0;32m     11\u001b[0m                          affinity\u001b[38;5;241m=\u001b[39maffinity,\n\u001b[0;32m     12\u001b[0m                          random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#clf = KMeans(n_clusters=n, max_iter=1000, n_init=1)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[0;32m     16\u001b[0m cluster_labels[i] \u001b[38;5;241m=\u001b[39m labels\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_spectral.py:691\u001b[0m, in \u001b[0;36mSpectralClustering.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    670\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform spectral clustering from features, or affinity matrix.\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \n\u001b[0;32m    672\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;124;03m        A fitted instance of the estimator.\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 691\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    692\u001b[0m         X,\n\u001b[0;32m    693\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    694\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m    695\u001b[0m         ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    696\u001b[0m     )\n\u001b[0;32m    697\u001b[0m     allow_squared \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffinity \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    699\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed_nearest_neighbors\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    700\u001b[0m     ]\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_squared:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:1087\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1087\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1088\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1091\u001b[0m         )\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1094\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 1 sample(s) (shape=(1, 6)) while a minimum of 2 is required by SpectralClustering."
     ]
    }
   ],
   "source": [
    "cluster_labels = get_clusters(X_matrix,\n",
    "                                 random_state=SEED,\n",
    "                                 **config['clustering'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEED': 10,\n",
       " 'clustering': {'affinity': 'cosine',\n",
       "  'count_max_clusters': 15,\n",
       "  'silhouette_metric': 'euclidean'},\n",
       " 'comments': {'YOUTUBE_API_KEY': 'AIzaSyCPYNxHdsk6_-UX60p9Hm65cPXWXifut9A',\n",
       "  'count_video': 5,\n",
       "  'limit': 30,\n",
       "  'maxResults': 20,\n",
       "  'nextPageToken': '',\n",
       "  'query': '–¥–∞—Ç–∞ —Å–∞–π–µ–Ω—Å'},\n",
       " 'cross_val': {'test_size': 0.3},\n",
       " 'dir_folder': '/Users/miracl6/airflow-mlflow-tutorial',\n",
       " 'model': {'class_weight': 'balanced'},\n",
       " 'model_lr': 'LogisticRegression',\n",
       " 'model_vec': 'vector_tfidf',\n",
       " 'name_experiment': 'my_first',\n",
       " 'stopwords': 'russian',\n",
       " 'tf_model': {'max_features': 300}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 3, 0, 3, 6, 0, 6, 0, 5], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_matrix,\n",
    "                                                    cluster_labels,\n",
    "                                                    **config['cross_val'],\n",
    "                                                    random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(**config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export MLFLOW_REGISTRY_URI=../mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'vector_tfidf' already exists. Creating a new version of this model...\n",
      "2021/05/02 19:53:43 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: vector_tfidf, version 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12980699 0.13219332 0.141417   0.12500189 0.12521816 0.11938949\n",
      "  0.11138085 0.1155923 ]\n",
      " [0.13225054 0.09461199 0.14431436 0.14921074 0.12746151 0.12149697\n",
      "  0.11319859 0.1174553 ]\n",
      " [0.1488154  0.09091486 0.15653628 0.14699363 0.12095594 0.11566808\n",
      "  0.10800232 0.11211349]\n",
      " [0.14166712 0.09029179 0.1767976  0.12378369 0.12853311 0.11464353\n",
      "  0.11344233 0.11084083]\n",
      " [0.14735099 0.09412684 0.14295515 0.12872404 0.12654938 0.12060076\n",
      "  0.12294893 0.11674391]\n",
      " [0.14852655 0.08947448 0.13987674 0.12342287 0.12322714 0.11324573\n",
      "  0.15235379 0.10987269]\n",
      " [0.1433901  0.09291234 0.14875431 0.12435637 0.12450994 0.14043332\n",
      "  0.11060968 0.11503393]\n",
      " [0.17761867 0.09219381 0.13923505 0.12629707 0.12340639 0.11763627\n",
      "  0.10961023 0.11400252]\n",
      " [0.13568458 0.0949781  0.15717993 0.13047939 0.12802549 0.12203414\n",
      "  0.11358408 0.11803429]\n",
      " [0.13182455 0.09333186 0.15002128 0.12495374 0.13229934 0.13286687\n",
      "  0.1191945  0.11550786]\n",
      " [0.14957397 0.09164231 0.15114628 0.12472615 0.12895741 0.12000372\n",
      "  0.1208868  0.11306337]\n",
      " [0.1392777  0.09596093 0.14717823 0.12958826 0.12980173 0.12360369\n",
      "  0.11503374 0.1195557 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '29' of model 'vector_tfidf'.\n",
      "Registered model 'LogisticRegression' already exists. Creating a new version of this model...\n",
      "2021/05/02 19:53:43 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: LogisticRegression, version 29\n",
      "Created version '29' of model 'LogisticRegression'.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment('comments')\n",
    "with mlflow.start_run():\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "    print(clf_lr.predict_proba(X_test))\n",
    "\n",
    "    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    mlflow.log_param(\n",
    "        'f1', get_f1_score(y_test, clf_lr.predict(X_test),\n",
    "                           set(cluster_labels)))\n",
    "    mlflow.sklearn.log_model(\n",
    "        tfidf,\n",
    "        artifact_path=\"vector\",\n",
    "        registered_model_name=f\"{config['model_vec']}\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf_lr,\n",
    "        artifact_path='model_lr',\n",
    "        registered_model_name=f\"{config['model_lr']}\")\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlflow/2/da17c6f5dbce43aeaa6727a3674d2376/artifacts'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_artifact_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_version_model(config_name, client):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏ –∏–∑ MLFlow\n",
    "    \"\"\"\n",
    "    dict_push = {}\n",
    "    for count, value in enumerate(\n",
    "        client.search_model_versions(f\"name='{config_name}'\")):\n",
    "        # client.list_registered_models()):\n",
    "        # –í—Å–µ –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏\n",
    "        dict_push[count] = value\n",
    "    return dict(list(dict_push.items())[-1][1])['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "last_version_lr = get_version_model(config['model_lr'], client)\n",
    "last_version_vec = get_version_model(config['model_vec'], client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_version_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_version_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
